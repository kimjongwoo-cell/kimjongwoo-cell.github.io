---
layout: post
title: Score-based Generative Model
date: 2024-03-29
description: NeurIPS19 - Generative Modeling by Estimating Gradients of the Data Distribution
tags: generative
categories: summary
comments: false
---

## Pique my Interests

한 때 이런 밈들이 유행하곤 했었다. 이 때까지는 사람들의 요청(prompt)를 받아, 합성 기술자(?)들이 기발한 아이디어를 곁들인 사진들을 생성(generative)해주곤 했었다.

<img width="700" src='{{"/assets/img/blog2_generative_score/intro0.png" | relative_url}}'>

요즘 이런 분야에서 Generative Model이라고 하는 것들이 등장해 (예전 합성 기술자(?)들의 기발함에는 한참 못 미치지만) 엄청난 성능을 보이고 있다고 한다.

<img width="700" src='{{"/assets/img/blog2_generative_score/intro1.png" | relative_url}}'>

그런데 더 놀라운 건 저런 사실적인 사진을 만드는데 사용되는 input이 이런 noise란다... 그래서 알아보게 되었다!

<img width="500" src='{{"/assets/img/blog2_generative_score/intro2.gif" | relative_url}}'>

### Big Picture

<figure>
<img width="700" src='{{"/assets/img/blog2_generative_score/distribution_learning.png" | relative_url}}'>
<figcaption><a href="https://youtu.be/nv-WTeKRLl0?feature=shared">Slide</a> from Yang Song</figcaption>
</figure>

들어가기에 앞서 큰 그림에서 Generative Model이 무엇을 학습하고자 하는 것인지 짚고 넘어가보자. 
\\
\\
Generative Model은 사진에 대한 분포(data distribution)를 학습하고자 한다. 예를 들어, 강아지 사진이라고 하면, 종(species) 별로 생김새가 비슷하므로 리트리버들은 서로 비슷한 영역(A)에 존재할 것이고, 허스키들 또한 많은 수의 사진이 무리지어 존재(B)할 것이다. 
\\
\\
그러나, 우리는 이 분포를 모른다(unknown)!
\\
\\
그래서, 이와 유사한 분포(model distribution)를 generative model을 통해 학습하려고 한다. \\
Model distribution이 Data distribution과 유사하게 잘 학습돠었다면, model distribution에서 샘플을 추출(sampling)할 때, 리트리버 또는 허스키 등이 존재하는 영역의 확률값이 높을 것(high probability)이므로 실제적인 리트리버 또는 허스키 사진이 생성(generation)되게 된다. \\
만약 확률값이 낮은 영역(low probability)에서의 샘플이 추출(sampling)된다면, 이상한 또는 어색한 사진이 생성(generation)될 것이다.

### Taxonomy

Generative Model을 크게 분류하면 아래와 같이 나눌 수 있다.

* Likelihood-based Model
    <img width="650" src='{{"/assets/img/blog2_generative_score/vae.png" | relative_url}}'>
    <img width="650" src='{{"/assets/img/blog2_generative_score/flow.png" | relative_url}}'>
    * Limitation: strong restrictions on the model architecture to ensure a tractable normalizing constant for likelihood computation, or must rely on surrogate objectives to approximate maximum likelihood training

* Generative Adversarial Networks (GANs)
    <img width="650" src='{{"/assets/img/blog2_generative_score/gan.png" | relative_url}}'>
    * Limitation: adversarial training, which is notoriously unstable and can lead to mode collapse

* <div style="color:red;">Energy-based Model</div>
    * Very flexible model architectures, stable training, relatively high sample quality, flexible composition

(credit: [source1](https://lilianweng.github.io/posts/2018-10-13-flow-models/), [source2](https://deepgenerativemodels.github.io/assets/slides/cs236_lecture11.pdf))

## Introduction

### Background

#### Energy-based Model (EBM)

Generative Model을 구축하기 위해 먼저 확률 분포를 나타내는 방법이 필요하다. 이를 위한 가장 직관적인 방법은 probability density function(pdf)을 직접 모델링하는 것이다. 아래와 같이 pdf를 정의해보자.

$$p_{\theta}(\mathbf{X}) = \frac{e^{\color{red}{-f_{\theta}(\mathbf{X})}}}{\color{limegreen}{Z_{\theta}}} \qquad \max_{\theta}\sum_{i=1}^{N}{\log{p_{\theta}(\mathbf{X}_i)}}$$

* Train $$p_{\theta}(\mathbf{X})$$ by maximizing the log-likelihood of the data
* $$\color{red}{f_{\theta}(\mathbf{X})} \in \mathbb{R}$$ is a real-valued function parameterized by a learnable parameter $$\theta$$
* $$\color{limegreen}{Z_{\theta}} > 0$$ is a normalizing constant dependent on $$\theta$$ such that $$\int{p_{\theta}(x)}\,dx = 1$$

여기서, function $$\color{red}{f_{\theta}(\mathbf{X})}$$는 보통 $$\color{red}{\text{normalized probabilistic model}}$$ 또는 $$\color{red}{\text{energy-based model}}$$이라고 불린다.
(credit: [source3](https://yang-song.net/blog/2021/score/), Advanced Machine Learning - Fall 2022 @KAIST)
\\
\\
그런데 왜 pdf는 저렇게 정의해야 할까?

* Parameterizing probability distributions $$p_{\theta}(\mathbf{X})$$
    * Non-negative: $$p_{\theta}(x) \ge 0$$
    * $$\color{red}{\text{Sum-to-one}}$$: $$\sum_{x}{p_{\theta}(x)}=1$$ or $$\int{p_{\theta}(x)}\,dx = 1$$

$$ \textcolor{blue}{\text{Energy-based Model (EBM)}} \qquad p_{\theta}{(\mathbf{X})} = \frac{1}{\int{\exp(f_{\theta}(\mathbf{X}))}\,dx}{\exp(f_{\theta}({\mathbf{X}}))} = \frac{1}{\underset{\substack{\textcolor{grey}{\text{partition}} \\ \textcolor{grey}{\text{function}}}}{\mathbf{Z}(\theta)}}{\exp{f_{\theta}({\mathbf{X}})}}$$

* Why exponential? - 그렇다면, 왜 exponential 을 사용할까?
    * 확률의 매우 큰 변동을 포착하고 싶기 때문이다 (log-probability는 natural scale). 그렇지 않으면, highly non-smooth 한 $$f_{\theta}$$가 필요하다.
    * 기본적으로 많은 분포들이 이러한 exponential family의 형식으로 표현될 수 있다.
    * 이러한 분포는 통계 물리학 (statistical physics) 에서 매우 일반적인 가정 하에 나타난다 (maximum entropy, second law of thermodynamics)
        * $$-f_{\theta}(\mathbf{X})$$ 는 **energy**라고 불리기 때문에 이름이 energy-based model인 것!
        * 직관적으로, 에너지가 낮은 (즉, 높은 $$f_{\theta}(\mathbf{X})$$) 구성 $$x$$가 더 가능성이 높다

    * Pros:
        * Extreme flexibility: can use pretty much any function $$f_{\theta}$$ you want
    * Cons:
        * $$p_{\theta}{(\mathbf{X})}$$ 에서 sampling을 하기 어렵다
        * Evaluating and optimizing likelihood $$p_{\theta}(\mathbf{X})$$ is hard (즉, 학습이 어렵다)
        * No feature learning (그러나, latent variables을 추가할 수는 있다)

**Curse of dimensionality**: 그러나 이러한 접근법의 근복적인 문제는 $$\mathbf{Z}(\theta)$$를 (가능한 analytic solution이 없을 때) 수치적으로 계산하는 것이 $$x$$의 차원을 기하급수적으로 커진다는데 있다. 그럼에도, 몇몇 작업은 $$\mathbf{Z}(\theta)$$를 **알 필요가 없다!**

(credit: [source4](https://deepgenerativemodels.github.io/assets/slides/cs236_lecture11.pdf))

#### Score-based Generative Models

여기서, 우리는 probability density function 대신에 **score function**을 모델링하므로써, intractable한 normalizing constant $$\mathbf{Z}(\theta)$$의 계산을 피할 수 있다!

$$\mathbf{s}_{\theta}(\mathbf{X}) = \nabla_{\mathbf{X}} \log{p_{\theta}(\mathbf{X})} = -\nabla_{\mathbf{X}}f_{\theta}(\mathbf{X}) - \cancelto{0}{\nabla_{\mathbf{X}} \log{\mathbf{Z}_{\theta}}} = -\nabla_{\mathbf{X}}f_{\theta}(\mathbf{X})$$

| <img width="200" align="center" src="/assets/img/blog2_generative_score/vectorfield.png"> | <img width="250" align="center" src="/assets/img/blog2_generative_score/pdf.gif"> | <img width="265" align="center" src="/assets/img/blog2_generative_score/score.gif"> |
| :------------: | :------------: | :-------------: |
| <small>Score function (the vector field) and density function (contours) of a mixture of two Gaussians</small> | <small>Parameterizing probability density functions. No matter how you change the model family and parameters, it has to be normalized (area under the curve must integrate to one)</small> | <small>Parameterizing score functions. No need to worry about normalization</small> |

(credit: [source5](https://yang-song.net/blog/2021/score/))

## Methodology

